use std::{collections::{HashMap, HashSet}, sync::Arc, time::{Duration, Instant}};

use anyhow::Result;
use tokio::{sync::Mutex, time::sleep};
use tracing::{info, warn, error};

use crate::{
    grpc::client::RaftClient,
    pb::{AppendEntriesRequest, AppendEntriesResponse, LogEntry},
    raft::node::{NodeRole, RaftNode},
};

/// æ—¥å¿—æ¡ç›®çŠ¶æ€ï¼ˆä½ è®¾è®¡çš„çŠ¶æ€æµè½¬ï¼‰
#[derive(Debug, Clone)]
pub enum LogEntryState {
    Local,                    // ä»…åœ¨Leaderæœ¬åœ°
    Replicating {            // æ­£åœ¨å¤åˆ¶ä¸­
        confirmed_nodes: HashSet<String>,
        required_confirmations: usize,
        retry_count: HashMap<String, usize>, // æ¯ä¸ªèŠ‚ç‚¹çš„é‡è¯•æ¬¡æ•°
    },
    Committed,               // å·²æäº¤ä½†æœªåº”ç”¨
    Applied,                 // å·²åº”ç”¨åˆ°çŠ¶æ€æœº
    Failed,                  // å¤åˆ¶å¤±è´¥ï¼ˆè¶…è¿‡é‡è¯•æ¬¡æ•°ï¼‰
}

/// å¤åˆ¶ä»»åŠ¡
#[derive(Debug)]
pub struct ReplicationTask {
    pub entry: LogEntry,
    pub target_nodes: Vec<String>,
    pub state: LogEntryState,
    pub created_at: Instant,
}

/// å¤åˆ¶ç»“æœ
#[derive(Debug)]
pub enum ReplicationResult {
    Success,                 // å¤åˆ¶æˆåŠŸ
    InProgress,             // ä»åœ¨è¿›è¡Œä¸­
    Failed(String),         // å¤åˆ¶å¤±è´¥
    ConsistencyError,       // ä¸€è‡´æ€§æ£€æŸ¥å¤±è´¥
}

/// æ—¥å¿—å¤åˆ¶æ¨¡å—
#[derive(Clone)]
pub struct LogReplication {
    client: Arc<Mutex<RaftClient>>,
    max_retry_count: usize,  // ä½ æåˆ°çš„3æ¬¡é‡è¯•é™åˆ¶
    retry_interval: Duration,
}

impl LogReplication {
    pub fn new(client: Arc<Mutex<RaftClient>>) -> Self {
        Self {
            client,
            max_retry_count: 3,
            retry_interval: Duration::from_millis(100),
        }
    }

    /// å¼€å§‹å¤åˆ¶å•ä¸ªæ—¥å¿—æ¡ç›®ï¼ˆä½ æåˆ°çš„å•æ¡å¤åˆ¶ï¼‰
    pub async fn replicate_entry(
        &self,
        node: Arc<Mutex<RaftNode>>,
        entry: LogEntry,
    ) -> Result<ReplicationResult> {
        let (leader_id, peers, current_term) = {
            let node_guard = node.lock().await;
            
            // åªæœ‰Leaderæ‰èƒ½å‘èµ·å¤åˆ¶
            if node_guard.role != NodeRole::Leader {
                return Err(anyhow::anyhow!("Only leader can replicate entries"));
            }
            
            (
                node_guard.node_id.clone(),
                node_guard.peers.clone(),
                node_guard.current_term,
            )
        };

        if peers.is_empty() {
            // å•èŠ‚ç‚¹é›†ç¾¤ï¼Œç›´æ¥æäº¤
            return Ok(ReplicationResult::Success);
        }

        let mut task = ReplicationTask {
            entry: entry.clone(),
            target_nodes: peers.clone(),
            state: LogEntryState::Replicating {
                confirmed_nodes: HashSet::new(),
                required_confirmations: peers.len() / 2 + 1,
                retry_count: HashMap::new(),
            },
            created_at: Instant::now(),
        };

        // æ‰§è¡Œå¤åˆ¶è¿‡ç¨‹
        self.execute_replication(node, &mut task, leader_id, current_term).await
    }

    /// æ‰§è¡Œå…·ä½“çš„å¤åˆ¶é€»è¾‘
    async fn execute_replication(
        &self,
        node: Arc<Mutex<RaftNode>>,
        task: &mut ReplicationTask,
        leader_id: String,
        current_term: u64,
    ) -> Result<ReplicationResult> {
        if let LogEntryState::Replicating { confirmed_nodes, required_confirmations, retry_count } = &mut task.state {
            
            // å¹¶å‘å‘é€åˆ°æ‰€æœ‰ç›®æ ‡èŠ‚ç‚¹
            let mut futures = Vec::new();
            
            for peer in &task.target_nodes {
                // æ£€æŸ¥æ˜¯å¦å·²ç»ç¡®è®¤æˆ–è¶…è¿‡é‡è¯•æ¬¡æ•°
                if confirmed_nodes.contains(peer) {
                    continue;
                }
                
                let current_retries = retry_count.get(peer).unwrap_or(&0);
                if *current_retries >= self.max_retry_count {
                    warn!("èŠ‚ç‚¹ {} è¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè·³è¿‡", peer);
                    continue;
                }

                let client = Arc::clone(&self.client);
                let peer_id = peer.clone();
                let entry = task.entry.clone();
                let leader_id_clone = leader_id.clone();
                let node_clone = Arc::clone(&node);

                let future = async move {
                    Self::send_append_entries(
                        client, 
                        node_clone,
                        peer_id.clone(), 
                        entry, 
                        leader_id_clone, 
                        current_term
                    ).await.map(|response| (peer_id, response))
                };
                
                futures.push(future);
            }

            // ç­‰å¾…æ‰€æœ‰å“åº”ï¼ˆä½ çš„å¹¶å‘ç­–ç•¥ï¼‰
            let results = futures::future::join_all(futures).await;

            // å¤„ç†å“åº”
            for result in results {
                match result {
                    Ok((peer_id, response)) => {
                        if self.handle_append_response(&peer_id, response, confirmed_nodes, retry_count).await {
                            info!("âœ… èŠ‚ç‚¹ {} ç¡®è®¤äº†æ—¥å¿—æ¡ç›® {}", peer_id, task.entry.index);
                        }
                    }
                    Err(e) => {
                        error!("å‘é€åˆ°èŠ‚ç‚¹å¤±è´¥: {}", e);
                        // å¢åŠ é‡è¯•è®¡æ•°
                        for peer in &task.target_nodes {
                            *retry_count.entry(peer.clone()).or_insert(0) += 1;
                        }
                    }
                }
            }

            // æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å¤šæ•°æ´¾
            if confirmed_nodes.len() >= *required_confirmations {
                task.state = LogEntryState::Committed;
                info!("ğŸ‰ æ—¥å¿—æ¡ç›® {} å·²è·å¾—å¤šæ•°æ´¾ç¡®è®¤", task.entry.index);
                return Ok(ReplicationResult::Success);
            }

            // æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å¯ä»¥é‡è¯•çš„èŠ‚ç‚¹
            let has_retryable_nodes = task.target_nodes.iter().any(|peer| {
                !confirmed_nodes.contains(peer) && 
                retry_count.get(peer).unwrap_or(&0) < &self.max_retry_count
            });

            if !has_retryable_nodes {
                task.state = LogEntryState::Failed;
                return Ok(ReplicationResult::Failed("æ‰€æœ‰èŠ‚ç‚¹éƒ½è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°".to_string()));
            }

            return Ok(ReplicationResult::InProgress);
        }

        Err(anyhow::anyhow!("Invalid replication state"))
    }

    /// å‘é€AppendEntriesè¯·æ±‚ï¼ˆå®ç°ä½ è¯´çš„ä¸€è‡´æ€§æ£€æŸ¥ï¼‰
    async fn send_append_entries(
        client: Arc<Mutex<RaftClient>>,
        node: Arc<Mutex<RaftNode>>,
        peer_id: String,
        entry: LogEntry,
        leader_id: String,
        current_term: u64,
    ) -> Result<AppendEntriesResponse> {
        // è·å–å‰ä¸€ä¸ªæ—¥å¿—æ¡ç›®çš„ä¿¡æ¯ç”¨äºä¸€è‡´æ€§æ£€æŸ¥
        let (prev_log_index, prev_log_term, leader_commit) = {
            let node_guard = node.lock().await;
            let prev_index = if entry.index > 1 { entry.index - 1 } else { 0 };
            let prev_term = if prev_index > 0 {
                // ä»æ—¥å¿—ä¸­è·å–å‰ä¸€ä¸ªæ¡ç›®çš„term
                node_guard.log.entities
                    .iter()
                    .find(|e| e.index == prev_index)
                    .map(|e| e.term)
                    .unwrap_or(0)
            } else {
                0
            };
            
            (prev_index, prev_term, node_guard.log.commit_index)
        };

        let _request = AppendEntriesRequest {
            term: current_term,
            leader_id: leader_id.clone(),
            prev_log_index,     // è¿™å°±æ˜¯ä½ è¯´çš„ä¸€è‡´æ€§æ£€æŸ¥å…³é”®
            prev_log_term,      // è¿™ä¸ªä¹Ÿæ˜¯
            entries: vec![entry.clone()],
            leader_commit,
        };

        // å‘é€è¯·æ±‚
        let _response = {
            let mut client_guard = client.lock().await;
            client_guard.send_request_vote(peer_id.clone(), tonic::Request::new(
                // TODO: è¿™é‡Œéœ€è¦ä¿®æ”¹clientæ¥å£æ”¯æŒAppendEntries
                crate::pb::VoteRequest {
                    term: current_term,
                    candidate_id: leader_id,
                    last_log_index: entry.index,
                    last_log_term: entry.term,
                }
            )).await?
        };

        // TODO: ä¸´æ—¶è¿”å›ï¼Œéœ€è¦å®ç°çœŸæ­£çš„AppendEntriesè°ƒç”¨
        Ok(AppendEntriesResponse {
            term: current_term,
            success: true,
            follower_id: peer_id,
            conflict_index: 0,
        })
    }

    /// å¤„ç†AppendEntrieså“åº”
    async fn handle_append_response(
        &self,
        peer_id: &str,
        response: AppendEntriesResponse,
        confirmed_nodes: &mut HashSet<String>,
        retry_count: &mut HashMap<String, usize>,
    ) -> bool {
        if response.success {
            confirmed_nodes.insert(peer_id.to_string());
            retry_count.remove(peer_id); // æˆåŠŸåæ¸…é™¤é‡è¯•è®¡æ•°
            true
        } else {
            // å¤±è´¥æ—¶å¢åŠ é‡è¯•è®¡æ•°ï¼ˆä½ çš„é‡è¯•ç­–ç•¥ï¼‰
            *retry_count.entry(peer_id.to_string()).or_insert(0) += 1;
            
            if response.conflict_index > 0 {
                // TODO: å®ç°å†²çªå¤„ç†é€»è¾‘ï¼ˆå›é€€next_indexï¼‰
                warn!("èŠ‚ç‚¹ {} æ—¥å¿—å†²çªï¼Œconflict_index: {}", peer_id, response.conflict_index);
            }
            
            false
        }
    }

    /// æ£€æŸ¥æ—¥å¿—ä¸€è‡´æ€§ï¼ˆä½ é—®çš„ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘ï¼‰
    pub fn check_log_consistency(
        local_log: &[LogEntry],
        prev_log_index: u64,
        prev_log_term: u64,
    ) -> bool {
        if prev_log_index == 0 {
            // è¿™æ˜¯ç¬¬ä¸€ä¸ªæ—¥å¿—æ¡ç›®ï¼Œæ€»æ˜¯ä¸€è‡´çš„
            return true;
        }

        // æŸ¥æ‰¾æŒ‡å®šç´¢å¼•çš„æ—¥å¿—æ¡ç›®
        if let Some(entry) = local_log.iter().find(|e| e.index == prev_log_index) {
            // æ£€æŸ¥termæ˜¯å¦åŒ¹é…
            entry.term == prev_log_term
        } else {
            // æœ¬åœ°æ²¡æœ‰è¯¥ç´¢å¼•çš„æ—¥å¿—ï¼Œä¸ä¸€è‡´
            false
        }
    }

    /// å¤„ç†æ—¥å¿—å†²çªï¼ˆä½ é—®çš„å›é€€æœºåˆ¶ï¼‰
    pub fn handle_log_conflict(
        next_index: &mut HashMap<String, u64>,
        peer_id: &str,
        conflict_index: u64,
    ) {
        if let Some(current_next) = next_index.get_mut(peer_id) {
            // ä½ å€¾å‘çš„è·³è·ƒå¼å›é€€ç­–ç•¥
            if conflict_index > 0 && conflict_index < *current_next {
                *current_next = conflict_index;
                info!("è°ƒæ•´ {} çš„next_indexåˆ° {}", peer_id, conflict_index);
            } else {
                // å®‰å…¨çš„çº¿æ€§å›é€€
                if *current_next > 1 {
                    *current_next -= 1;
                }
                info!("çº¿æ€§å›é€€ {} çš„next_indexåˆ° {}", peer_id, *current_next);
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use std::time::Instant;

    use crate::{
        pb::LogEntry,
        raft::{log::RaftLog, state_machine::ConfigStateMachine},
    };

    fn create_test_entry(index: u64, term: u64, data: &str) -> LogEntry {
        LogEntry {
            index,
            term,
            data: data.as_bytes().to_vec(),
            entry_type: "config".to_string(),
            key: format!("key_{}", index),
        }
    }

    #[test]
    fn test_log_consistency_check() {
        // æµ‹è¯•ä½ é—®çš„ä¸€è‡´æ€§æ£€æŸ¥é€»è¾‘
        let logs = vec![
            create_test_entry(1, 1, "data1"),
            create_test_entry(2, 1, "data2"),
            create_test_entry(3, 2, "data3"),
        ];

        // æµ‹è¯•ç¬¬ä¸€ä¸ªæ¡ç›®ï¼ˆprev_log_index=0ï¼‰
        assert!(LogReplication::check_log_consistency(&logs, 0, 0));

        // æµ‹è¯•æ­£å¸¸åŒ¹é…çš„æƒ…å†µ
        assert!(LogReplication::check_log_consistency(&logs, 1, 1));
        assert!(LogReplication::check_log_consistency(&logs, 2, 1));
        assert!(LogReplication::check_log_consistency(&logs, 3, 2));

        // æµ‹è¯•termä¸åŒ¹é…çš„æƒ…å†µ
        assert!(!LogReplication::check_log_consistency(&logs, 1, 2));
        assert!(!LogReplication::check_log_consistency(&logs, 2, 2));

        // æµ‹è¯•ç´¢å¼•ä¸å­˜åœ¨çš„æƒ…å†µ
        assert!(!LogReplication::check_log_consistency(&logs, 4, 2));
        assert!(!LogReplication::check_log_consistency(&logs, 10, 1));
    }

    #[test]
    fn test_log_conflict_handling() {
        // æµ‹è¯•ä½ é—®çš„å›é€€æœºåˆ¶
        
        // æµ‹è¯•1: è·³è·ƒå¼å›é€€ï¼ˆä½ å€¾å‘çš„ç­–ç•¥ï¼‰
        let mut next_index = HashMap::new();
        next_index.insert("node1".to_string(), 5);
        LogReplication::handle_log_conflict(&mut next_index, "node1", 3);
        assert_eq!(next_index.get("node1"), Some(&3));

        // æµ‹è¯•2: conflict_indexæ— æ•ˆæ—¶çš„çº¿æ€§å›é€€
        let mut next_index = HashMap::new();
        next_index.insert("node2".to_string(), 3);
        LogReplication::handle_log_conflict(&mut next_index, "node2", 0);
        assert_eq!(next_index.get("node2"), Some(&2));

        // æµ‹è¯•3: conflict_indexå¤§äºcurrentçš„æƒ…å†µï¼ˆåº”è¯¥çº¿æ€§å›é€€ï¼‰
        let mut next_index = HashMap::new();
        next_index.insert("node1".to_string(), 3);
        LogReplication::handle_log_conflict(&mut next_index, "node1", 10);
        assert_eq!(next_index.get("node1"), Some(&2)); // åº”è¯¥çº¿æ€§å›é€€

        // æµ‹è¯•4: æœ€å°å€¼è¾¹ç•Œ
        let mut next_index = HashMap::new();
        next_index.insert("node3".to_string(), 1);
        LogReplication::handle_log_conflict(&mut next_index, "node3", 0);
        assert_eq!(next_index.get("node3"), Some(&1)); // ä¸åº”è¯¥å°äº1
    }

    #[test]
    fn test_replication_state_transitions() {
        // æµ‹è¯•ä½ è®¾è®¡çš„çŠ¶æ€æµè½¬
        let mut state = LogEntryState::Local;

        // Local -> Replicating
        state = LogEntryState::Replicating {
            confirmed_nodes: HashSet::new(),
            required_confirmations: 3,
            retry_count: HashMap::new(),
        };

        if let LogEntryState::Replicating { confirmed_nodes, .. } = &mut state {
            confirmed_nodes.insert("node1".to_string());
            confirmed_nodes.insert("node2".to_string());
        }

        // Replicating -> Committed
        state = LogEntryState::Committed;
        assert!(matches!(state, LogEntryState::Committed));

        // Committed -> Applied
        state = LogEntryState::Applied;
        assert!(matches!(state, LogEntryState::Applied));
    }

    #[test]
    fn test_retry_count_tracking() {
        // æµ‹è¯•ä½ æåˆ°çš„3æ¬¡é‡è¯•æœºåˆ¶
        let mut retry_count = HashMap::new();
        
        // æ¨¡æ‹Ÿé‡è¯•è¿‡ç¨‹
        for i in 1..=3 {
            *retry_count.entry("node1".to_string()).or_insert(0) += 1;
            let current_retries = retry_count.get("node1").unwrap();
            
            if i < 3 {
                assert!(*current_retries < 3, "ç¬¬{}æ¬¡é‡è¯•ï¼Œåº”è¯¥è¿˜å¯ä»¥ç»§ç»­", i);
            } else {
                assert!(*current_retries >= 3, "ç¬¬{}æ¬¡é‡è¯•ï¼Œåº”è¯¥è¾¾åˆ°ä¸Šé™", i);
            }
        }

        // è¶…è¿‡ä¸Šé™åä¸åº”è¯¥å†é‡è¯•
        assert_eq!(retry_count.get("node1"), Some(&3));
    }

    #[test]
    fn test_majority_calculation() {
        // æµ‹è¯•å¤šæ•°æ´¾è®¡ç®—ï¼ˆä¸ä½ çš„é€‰ä¸¾æµ‹è¯•ç±»ä¼¼ï¼‰
        assert_eq!(calculate_majority_for_replication(1), 1);
        assert_eq!(calculate_majority_for_replication(2), 2);
        assert_eq!(calculate_majority_for_replication(3), 2);
        assert_eq!(calculate_majority_for_replication(4), 3);
        assert_eq!(calculate_majority_for_replication(5), 3);
    }

    #[test]
    fn test_replication_task_creation() {
        // æµ‹è¯•å¤åˆ¶ä»»åŠ¡çš„åˆ›å»º
        let entry = create_test_entry(1, 1, "test_data");
        let peers = vec!["node2".to_string(), "node3".to_string(), "node4".to_string()];
        
        let task = ReplicationTask {
            entry: entry.clone(),
            target_nodes: peers.clone(),
            state: LogEntryState::Replicating {
                confirmed_nodes: HashSet::new(),
                required_confirmations: peers.len() / 2 + 1, // åº”è¯¥æ˜¯2
                retry_count: HashMap::new(),
            },
            created_at: Instant::now(),
        };

        assert_eq!(task.entry.index, 1);
        assert_eq!(task.target_nodes.len(), 3);
        
        if let LogEntryState::Replicating { required_confirmations, .. } = task.state {
            assert_eq!(required_confirmations, 2);
        } else {
            panic!("çŠ¶æ€åº”è¯¥æ˜¯Replicating");
        }
    }

    // è¾…åŠ©å‡½æ•°
    fn calculate_majority_for_replication(total_nodes: usize) -> usize {
        total_nodes / 2 + 1
    }
}
