use anyhow::{anyhow, Result};
use serde::{Deserialize, Serialize};
use std::fs;
use std::path::Path;
use tracing::{info, warn};

/// é›†ç¾¤é…ç½® - ç±»ä¼¼ Hadoop çš„ Configuration ç±»
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClusterConfig {
    pub cluster: ClusterInfo,
    pub nodes: Vec<NodeConfig>,
    pub raft: RaftConfig,
    pub storage: StorageConfig,
    pub network: NetworkConfig,
    pub logging: LoggingConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ClusterInfo {
    pub name: String,
    pub version: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NodeConfig {
    pub id: String,
    pub host: String,
    pub port: u16,
    pub grpc_port: u16,
    pub data_dir: String,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RaftConfig {
    pub election_timeout_min: u64,
    pub election_timeout_max: u64,
    pub heartbeat_interval: u64,
    pub log_compaction: LogCompactionConfig,
    pub client_timeout: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogCompactionConfig {
    pub max_log_entries: usize,
    pub snapshot_threshold: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct StorageConfig {
    pub persistence: PersistenceConfig,
    pub log: LogStorageConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct PersistenceConfig {
    pub enabled: bool,
    pub sync_on_write: bool,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LogStorageConfig {
    pub max_size_mb: usize,
    pub rotation_count: usize,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct NetworkConfig {
    pub connect_timeout: u64,
    pub read_timeout: u64,
    pub write_timeout: u64,
    pub retry: RetryConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct RetryConfig {
    pub max_attempts: usize,
    pub backoff_ms: u64,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct LoggingConfig {
    pub level: String,
    pub format: String,
    pub output: String,
    pub file: FileLoggingConfig,
}

#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct FileLoggingConfig {
    pub enabled: bool,
    pub path: String,
    pub max_size_mb: usize,
    pub max_files: usize,
}

/// é…ç½®åŠ è½½å™¨ - ç±»ä¼¼ Hadoop çš„ Configuration.addResource()
pub struct ConfigLoader;

impl ConfigLoader {
    /// ä»YAMLæ–‡ä»¶åŠ è½½é…ç½®
    pub fn load_from_yaml<P: AsRef<Path>>(path: P) -> Result<ClusterConfig> {
        let path = path.as_ref();
        info!("ğŸ“ æ­£åœ¨åŠ è½½é›†ç¾¤é…ç½®æ–‡ä»¶: {}", path.display());

        if !path.exists() {
            return Err(anyhow!("é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {}", path.display()));
        }

        let content = fs::read_to_string(path)
            .map_err(|e| anyhow!("è¯»å–é…ç½®æ–‡ä»¶å¤±è´¥: {}", e))?;

        let config: ClusterConfig = serde_yaml::from_str(&content)
            .map_err(|e| anyhow!("è§£æYAMLé…ç½®å¤±è´¥: {}", e))?;

        Self::validate_config(&config)?;
        
        info!("âœ… é›†ç¾¤é…ç½®åŠ è½½æˆåŠŸ: {} (ç‰ˆæœ¬: {})", 
              config.cluster.name, config.cluster.version);
        info!("ğŸŒ å‘ç° {} ä¸ªèŠ‚ç‚¹", config.nodes.len());
        
        Ok(config)
    }

    /// ä»ç¯å¢ƒå˜é‡åŠ è½½é…ç½® (ç±»ä¼¼ Hadoop çš„ç¯å¢ƒå˜é‡è¦†ç›–)
    pub fn load_from_env(base_config: &mut ClusterConfig) -> Result<()> {
        info!("ğŸ”§ æ£€æŸ¥ç¯å¢ƒå˜é‡é…ç½®è¦†ç›–...");

        // è¦†ç›–é›†ç¾¤åç§°
        if let Ok(cluster_name) = std::env::var("RAFT_CLUSTER_NAME") {
            warn!("ğŸ”„ ç¯å¢ƒå˜é‡è¦†ç›–é›†ç¾¤åç§°: {} -> {}", 
                  base_config.cluster.name, cluster_name);
            base_config.cluster.name = cluster_name;
        }

        // è¦†ç›–æ—¥å¿—çº§åˆ«
        if let Ok(log_level) = std::env::var("RAFT_LOG_LEVEL") {
            warn!("ğŸ”„ ç¯å¢ƒå˜é‡è¦†ç›–æ—¥å¿—çº§åˆ«: {} -> {}", 
                  base_config.logging.level, log_level);
            base_config.logging.level = log_level;
        }

        // è¦†ç›–é€‰ä¸¾è¶…æ—¶
        if let Ok(timeout_str) = std::env::var("RAFT_ELECTION_TIMEOUT_MIN") {
            if let Ok(timeout) = timeout_str.parse::<u64>() {
                warn!("ğŸ”„ ç¯å¢ƒå˜é‡è¦†ç›–é€‰ä¸¾è¶…æ—¶æœ€å°å€¼: {} -> {}", 
                      base_config.raft.election_timeout_min, timeout);
                base_config.raft.election_timeout_min = timeout;
            }
        }

        if let Ok(timeout_str) = std::env::var("RAFT_ELECTION_TIMEOUT_MAX") {
            if let Ok(timeout) = timeout_str.parse::<u64>() {
                warn!("ğŸ”„ ç¯å¢ƒå˜é‡è¦†ç›–é€‰ä¸¾è¶…æ—¶æœ€å¤§å€¼: {} -> {}", 
                      base_config.raft.election_timeout_max, timeout);
                base_config.raft.election_timeout_max = timeout;
            }
        }

        info!("âœ… ç¯å¢ƒå˜é‡é…ç½®åŠ è½½å®Œæˆ");
        Ok(())
    }

    /// éªŒè¯é…ç½®åˆæ³•æ€§
    fn validate_config(config: &ClusterConfig) -> Result<()> {
        info!("ğŸ” éªŒè¯é›†ç¾¤é…ç½®...");

        // éªŒè¯èŠ‚ç‚¹é…ç½®
        if config.nodes.is_empty() {
            return Err(anyhow!("é›†ç¾¤å¿…é¡»è‡³å°‘åŒ…å«ä¸€ä¸ªèŠ‚ç‚¹"));
        }

        // éªŒè¯èŠ‚ç‚¹IDå”¯ä¸€æ€§
        let mut node_ids = std::collections::HashSet::new();
        for node in &config.nodes {
            if !node_ids.insert(&node.id) {
                return Err(anyhow!("å‘ç°é‡å¤çš„èŠ‚ç‚¹ID: {}", node.id));
            }
        }

        // éªŒè¯ç«¯å£å”¯ä¸€æ€§
        let mut ports = std::collections::HashSet::new();
        for node in &config.nodes {
            let address = format!("{}:{}", node.host, node.port);
            if !ports.insert(address.clone()) {
                return Err(anyhow!("å‘ç°é‡å¤çš„èŠ‚ç‚¹åœ°å€: {}", address));
            }
        }

        // éªŒè¯Rafté…ç½®
        if config.raft.election_timeout_min >= config.raft.election_timeout_max {
            return Err(anyhow!("é€‰ä¸¾è¶…æ—¶æœ€å°å€¼å¿…é¡»å°äºæœ€å¤§å€¼"));
        }

        if config.raft.heartbeat_interval >= config.raft.election_timeout_min {
            return Err(anyhow!("å¿ƒè·³é—´éš”å¿…é¡»å°äºé€‰ä¸¾è¶…æ—¶æœ€å°å€¼"));
        }

        info!("âœ… é…ç½®éªŒè¯é€šè¿‡");
        Ok(())
    }
}

impl ClusterConfig {
    /// è·å–å½“å‰èŠ‚ç‚¹é…ç½®
    pub fn get_node_config(&self, node_id: &str) -> Option<&NodeConfig> {
        self.nodes.iter().find(|node| node.id == node_id)
    }

    /// è·å–å…¶ä»–èŠ‚ç‚¹åˆ—è¡¨ (ç±»ä¼¼ Hadoop çš„ getSlaves())
    pub fn get_peer_nodes(&self, current_node_id: &str) -> Vec<&NodeConfig> {
        self.nodes.iter()
            .filter(|node| node.id != current_node_id)
            .collect()
    }

    /// è·å–æ‰€æœ‰èŠ‚ç‚¹çš„gRPCåœ°å€
    pub fn get_all_grpc_addresses(&self) -> Vec<String> {
        self.nodes.iter()
            .map(|node| format!("http://{}:{}", node.host, node.grpc_port))
            .collect()
    }

    /// è·å–peerèŠ‚ç‚¹çš„gRPCåœ°å€
    pub fn get_peer_grpc_addresses(&self, current_node_id: &str) -> Vec<String> {
        self.get_peer_nodes(current_node_id)
            .into_iter()
            .map(|node| format!("http://{}:{}", node.host, node.grpc_port))
            .collect()
    }

    /// åˆ›å»ºæ•°æ®ç›®å½• (ç±»ä¼¼ Hadoop çš„æ•°æ®ç›®å½•åˆå§‹åŒ–)
    pub fn ensure_data_directories(&self) -> Result<()> {
        info!("ğŸ“ ç¡®ä¿æ‰€æœ‰æ•°æ®ç›®å½•å­˜åœ¨...");
        
        for node in &self.nodes {
            let data_dir = Path::new(&node.data_dir);
            if !data_dir.exists() {
                fs::create_dir_all(data_dir)
                    .map_err(|e| anyhow!("åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥ {}: {}", data_dir.display(), e))?;
                info!("ğŸ“ åˆ›å»ºæ•°æ®ç›®å½•: {}", data_dir.display());
            }
        }

        info!("âœ… æ‰€æœ‰æ•°æ®ç›®å½•æ£€æŸ¥å®Œæˆ");
        Ok(())
    }
} 